{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T19:12:39.756051Z","iopub.execute_input":"2024-04-05T19:12:39.756460Z","iopub.status.idle":"2024-04-05T19:12:41.090607Z","shell.execute_reply.started":"2024-04-05T19:12:39.756428Z","shell.execute_reply":"2024-04-05T19:12:41.088930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:41.092875Z","iopub.execute_input":"2024-04-05T19:12:41.093414Z","iopub.status.idle":"2024-04-05T19:12:44.475388Z","shell.execute_reply.started":"2024-04-05T19:12:41.093379Z","shell.execute_reply":"2024-04-05T19:12:44.471463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # 1. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Property Characteristics\n\n**Location:**\n\n* **MSZoning:** Zoning code (residential, commercial, industrial)\n* **Neighborhood:** Area within Ames where the house is located\n* **Street:** Type of road the property faces (highway, cul-de-sac, etc.)\n* **Alley:** Does the property have an alley for back access?\n\n**Lot:**\n\n* **LotFrontage:** Length in feet of the street side of the property\n* **LotArea:** Total size of the property in square feet\n* **LotShape:** How the property is shaped (rectangular, irregular, etc.)\n* **LandContour:** How flat or hilly the property is\n* **LotConfig:** How the property is laid out (corner lot, inside lot, etc.)\n* **LandSlope:** The slope of the land the house sits on\n\n**Size and Living Space:**\n\n* **OverallQual/OverallCond:** General rating of the house's quality and condition (high = good, low = bad)\n* **YearBuilt/YearRemodAdd:** Year the house was built and any major renovations done\n* **GrLivArea:** Total finished living area above ground in square feet\n* **TotalBsmtSF:** Total size of the basement area in square feet (finished and unfinished)\n* **1stFlrSF/2ndFlrSF:** Square footage of the first and second floors (if applicable)\n* **FullBath/HalfBath:** Number of full and half bathrooms above ground\n* **Bedroom/Kitchen:** Number of bedrooms and kitchens\n* **TotRmsAbvGrd:** Total number of rooms above ground (excluding bathrooms)\n* **Functional:** Overall rating of the house's layout and functionality\n\n**Building Materials and Finishes:**\n\n* **ExterQual/ExterCond:** Quality and condition of the exterior siding and materials\n* **Foundation:** Type of foundation the house sits on (concrete slab, basement, etc.)\n* **BsmtQual/BsmtCond:** Quality and condition of the basement (unfinished, finished, etc.)\n* **RoofStyle/RoofMatl:** Style and material of the roof (shingles, metal, etc.)\n* **Exterior1st/Exterior2nd:** Main and secondary exterior siding materials (brick, vinyl, etc.)\n* **MasVnrType/MasVnrArea:** Type and amount of masonry veneer (decorative stonework) on the exterior\n* **Heating/HeatingQC:** Type of heating system (furnace, boiler, etc.) and its condition\n* **CentralAir:** Does the house have central air conditioning?\n* **Electrical:** Type of electrical wiring system in the house\n* **KitchenQual:** Quality of the kitchen cabinets and finishes\n* **GarageQual/GarageCond:** Quality and condition of the garage (if present)\n* **Fence:** Does the property have a fence, and if so, what quality is it?\n\n## Amenities:\n\n* **Fireplace/FireplaceQu:** Does the house have a fireplace, and if so, what quality is it?\n* **PoolArea/PoolQC:** Does the property have a pool, and if so, what quality is it?\n* **WoodDeckSF/OpenPorchSF/EnclosedPorch/ScreenPorch/3SsnPorch:** Square footage of various types of porches (wooden deck, open porch, etc.)\n\n## Sale Related Information:\n\n* **MoSold/YrSold:** Month and year the house was sold\n* **SaleType:** Type of sale (traditional, auction, short sale, etc.)\n* **SaleCondition:** Condition of the sale (typical, abnormal, distressed, etc.)\n","metadata":{}},{"cell_type":"markdown","source":"## 1.2 Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the *Ames* competition dataset, we'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"## 1.2.1 Load Data","metadata":{}},{"cell_type":"code","source":"def load_data():\n  #removing outliers per the information file guidance\n    \n    # Read data\n    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.484134Z","iopub.execute_input":"2024-04-05T19:12:44.484596Z","iopub.status.idle":"2024-04-05T19:12:44.494848Z","shell.execute_reply.started":"2024-04-05T19:12:44.484552Z","shell.execute_reply":"2024-04-05T19:12:44.493347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2.2 Clean Data","metadata":{}},{"cell_type":"markdown","source":"#### Rectifying Categorical Features:\n\n- **Exterior2nd Correction:** \n  - Replaced \"Brk Cmn\" with \"BrkComm\" to standardize the category naming.\n\n#### Handling Corrupt Garage Year Built Data:\n- **GarageYrBlt Correction:** \n  - Replaced corrupt values with the respective year of house construction to maintain data integrity.\n\n#### Improved Clarity in Column Names:\n- **Column Name Updates:** \n  - Renamed columns beginning with numbers to improve readability and usability:\n    - \"1stFlrSF\" ⟶ \"FirstFlrSF\"\n    - \"2ndFlrSF\" ⟶ \"SecondFlrSF\"\n    - \"3SsnPorch\" ⟶ \"Threeseasonporch\"\n\nThese cleaning steps enhance data consistency, integrity, and ease of interpretation.\n","metadata":{}},{"cell_type":"code","source":"data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\ndf = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n\ndf.Exterior2nd.unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.497426Z","iopub.execute_input":"2024-04-05T19:12:44.497960Z","iopub.status.idle":"2024-04-05T19:12:44.593432Z","shell.execute_reply.started":"2024-04-05T19:12:44.497903Z","shell.execute_reply":"2024-04-05T19:12:44.592037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them\n    # with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\",\n    }, inplace=True,\n    )\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.595393Z","iopub.execute_input":"2024-04-05T19:12:44.595870Z","iopub.status.idle":"2024-04-05T19:12:44.605341Z","shell.execute_reply.started":"2024-04-05T19:12:44.595826Z","shell.execute_reply":"2024-04-05T19:12:44.603913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2.3 Categorical Feature Encoding","metadata":{}},{"cell_type":"markdown","source":"To properly handle categorical features in the dataset, the following encoding process has been implemented:\n\n#### Nominal (Unordered) Categorical Features:\n- The specified nominal features have been converted to categorical data types.\n- A \"None\" category has been added for missing values.\n\n#### Ordinal (Ordered) Categorical Features:\n- Ordinal features with predefined levels have been encoded using ordered categorical data types.\n- A \"None\" level has been added for missing values.\n\nThis encoding ensures proper treatment of categorical features, facilitating downstream analysis and modeling tasks.\n","metadata":{}},{"cell_type":"code","source":"# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name] = df[name].cat.add_categories(\"None\")\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.607079Z","iopub.execute_input":"2024-04-05T19:12:44.607569Z","iopub.status.idle":"2024-04-05T19:12:44.627418Z","shell.execute_reply.started":"2024-04-05T19:12:44.607525Z","shell.execute_reply":"2024-04-05T19:12:44.625732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2.4 Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","metadata":{}},{"cell_type":"code","source":"def impute(df):\n    # Find numerical and categorical columns (excluding SalePrice)\n    numerical_columns = df.select_dtypes(include=['int64', 'float64']).drop(columns=['SalePrice']).columns.tolist()\n    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n    # Fill missing values for numerical columns with median\n    for col in numerical_columns:\n        median_value = df[col].median()\n        df[col].fillna(median_value, inplace=True)\n\n    # Fill missing values for categorical columns with mode\n    for col in categorical_columns:\n        mode_value = df[col].mode()[0]  # mode() returns a DataFrame, so we select the first value\n        df[col].fillna(mode_value, inplace=True)\n    \n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.629690Z","iopub.execute_input":"2024-04-05T19:12:44.630273Z","iopub.status.idle":"2024-04-05T19:12:44.643485Z","shell.execute_reply.started":"2024-04-05T19:12:44.630225Z","shell.execute_reply":"2024-04-05T19:12:44.642241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2.5 Load Data","metadata":{}},{"cell_type":"code","source":"df_train, df_test = load_data()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.645299Z","iopub.execute_input":"2024-04-05T19:12:44.645704Z","iopub.status.idle":"2024-04-05T19:12:44.858328Z","shell.execute_reply.started":"2024-04-05T19:12:44.645658Z","shell.execute_reply":"2024-04-05T19:12:44.856958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.863996Z","iopub.execute_input":"2024-04-05T19:12:44.865098Z","iopub.status.idle":"2024-04-05T19:12:44.873952Z","shell.execute_reply.started":"2024-04-05T19:12:44.865046Z","shell.execute_reply":"2024-04-05T19:12:44.872831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:44.875304Z","iopub.execute_input":"2024-04-05T19:12:44.875757Z","iopub.status.idle":"2024-04-05T19:12:48.594412Z","shell.execute_reply.started":"2024-04-05T19:12:44.875723Z","shell.execute_reply":"2024-04-05T19:12:48.593379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2.6 Conclusion","metadata":{}},{"cell_type":"markdown","source":" `I noticed the impact of the preprocessing strategy outlined—specifically, encoding categorical data before handling missing values and explicitly adding a 'None' category for missing values can significantly improve model performance.` ","metadata":{}},{"cell_type":"markdown","source":"#### Informative Missingness: \nTreating \"None\" as a distinct category can highlight the significance of missing data, like a property lacking a garage, offering unique insights that may enhance prediction accuracy.\n\n#### Data Integrity Preservation: \nEncoding missing values as \"None\" utilizes all data without resorting to deletion or arbitrary imputation, potentially enriching the dataset's representation and improving model outcomes.","metadata":{}},{"cell_type":"markdown","source":"# 2. Feature Utility Scores","metadata":{}},{"cell_type":"markdown","source":"Feature utility scores, such as `Mutual Information (MI) scores`, quantify the relationship between a feature and the target variable. MI scores guide feature selection by\nidentifying features with predictive power. They are computed using `make_mi_scores` and visualized with `plot_mi_scores`.","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Extracting Important Features using MI Scores","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\n\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n        \n    num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n    imputer = SimpleImputer(strategy=\"median\")\n    X[num_cols] = imputer.fit_transform(X[num_cols])\n    \n    # Correct argument name from 'dis_features' to 'discrete_features'\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    \n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nmi_scores = make_mi_scores(X, y)\nprint(mi_scores)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:48.599929Z","iopub.execute_input":"2024-04-05T19:12:48.602628Z","iopub.status.idle":"2024-04-05T19:12:49.527311Z","shell.execute_reply.started":"2024-04-05T19:12:48.602579Z","shell.execute_reply":"2024-04-05T19:12:49.526134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 9))\nmi_scores.plot.bar()\nplt.title('Mutual Informtion Scores')\nplt.ylabel('MI Scores')\nplt.xlabel('Features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:49.528820Z","iopub.execute_input":"2024-04-05T19:12:49.529965Z","iopub.status.idle":"2024-04-05T19:12:50.848248Z","shell.execute_reply.started":"2024-04-05T19:12:49.529921Z","shell.execute_reply":"2024-04-05T19:12:50.847326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing MI Scores\n##### Inspect Top Features:\n\nLook at the features with the highest `MI scores`.\nThese features are the most informative and are likely to be the most beneficial for predictive modeling.\n\n##### Set a Threshold:\n\nYou can set a `threshold MI score` based on domain knowledge or experimentation.\n\nFeatures with MI scores above this threshold are considered good predictors, while those below may be less useful.","metadata":{}},{"cell_type":"code","source":"print(mi_scores.head(10))\n\nthreshold = 0.05\n\ngood_scores = mi_scores[mi_scores > threshold]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:50.849567Z","iopub.execute_input":"2024-04-05T19:12:50.850115Z","iopub.status.idle":"2024-04-05T19:12:50.858105Z","shell.execute_reply.started":"2024-04-05T19:12:50.850081Z","shell.execute_reply":"2024-04-05T19:12:50.856948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mi_scores[mi_scores > 0.05])","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:50.859676Z","iopub.execute_input":"2024-04-05T19:12:50.860707Z","iopub.status.idle":"2024-04-05T19:12:50.871172Z","shell.execute_reply.started":"2024-04-05T19:12:50.860664Z","shell.execute_reply":"2024-04-05T19:12:50.869797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Drop Uninformative Features: \n   - A function `drop_uninformative(df, mi_scores)` is defined to drop features from the DataFrame (`df`) that have MI scores less than or equal to 0.05. \n   - The function takes the DataFrame and MI scores as inputs, identifies the common columns between the DataFrame and MI scores using `intersection`, selects the corresponding MI scores for the common columns, and aligns them with the DataFrame columns. \n   - Features with MI scores greater than 0.05 are retained in the DataFrame using boolean indexing.","metadata":{}},{"cell_type":"code","source":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:50.873212Z","iopub.execute_input":"2024-04-05T19:12:50.873619Z","iopub.status.idle":"2024-04-05T19:12:50.881224Z","shell.execute_reply.started":"2024-04-05T19:12:50.873586Z","shell.execute_reply":"2024-04-05T19:12:50.879907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)\n\nscore_dataset(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:50.882816Z","iopub.execute_input":"2024-04-05T19:12:50.883803Z","iopub.status.idle":"2024-04-05T19:12:52.757916Z","shell.execute_reply.started":"2024-04-05T19:12:50.883763Z","shell.execute_reply":"2024-04-05T19:12:52.756935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Label Encoding","metadata":{}},{"cell_type":"markdown","source":" `Label encoding is a process used in machine learning to convert categorical data into numerical format by assigning a unique integer to each category.`\n This transformation allows machine learning algorithms to effectively process and analyze categorical variables, especially when working with models that require numerical input.","metadata":{}},{"cell_type":"markdown","source":"### 2.3.1 How It Is Different From One-Hot Encoding:\n`In label encoding, ordinal relationships signify the inherent order among categories, such as \"low,\" \"medium,\" and \"high,\" represented by integer values (e.g., 0, 1, and 2), while one-hot encoding avoids such assumptions by creating binary columns for each category, maintaining independence.`","metadata":{}},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.762340Z","iopub.execute_input":"2024-04-05T19:12:52.765630Z","iopub.status.idle":"2024-04-05T19:12:52.772336Z","shell.execute_reply.started":"2024-04-05T19:12:52.765584Z","shell.execute_reply":"2024-04-05T19:12:52.771284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Create Features","metadata":{}},{"cell_type":"markdown","source":"`Creating features involves the process of transforming or deriving new variables from existing data, aiming to enhance the predictive power and interpretability of machine learning models.`","metadata":{}},{"cell_type":"code","source":"print(df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.774148Z","iopub.execute_input":"2024-04-05T19:12:52.774622Z","iopub.status.idle":"2024-04-05T19:12:52.785565Z","shell.execute_reply.started":"2024-04-05T19:12:52.774576Z","shell.execute_reply":"2024-04-05T19:12:52.784569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n    # This feature ended up not helping performance\n    # X[\"TotalOutsideSF\"] = \\\n    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n    #     df.Threeseasonporch + df.ScreenPorch\n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X\n\n\n\ndef interaction_3(df):\n    # Fill missing values with median for LotArea, GarageArea, and PoolArea columns\n    df[\"LotArea\"].fillna(df[\"LotArea\"].median(), inplace=True)\n    df[\"GarageArea\"].fillna(df[\"GarageArea\"].median(), inplace=True)\n    df[\"PoolArea\"].fillna(df[\"PoolArea\"].median(), inplace=True)\n    \n    X = pd.DataFrame()\n    X[\"LotArea\"] = np.sqrt(df[\"LotArea\"])\n    X[\"GarageArea\"] = np.sqrt(df[\"GarageArea\"])\n    X[\"PoolArea\"] = np.sqrt(df[\"PoolArea\"])\n    \n    return X\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.787171Z","iopub.execute_input":"2024-04-05T19:12:52.787548Z","iopub.status.idle":"2024-04-05T19:12:52.804372Z","shell.execute_reply.started":"2024-04-05T19:12:52.787515Z","shell.execute_reply":"2024-04-05T19:12:52.803021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Skewness of variables","metadata":{}},{"cell_type":"code","source":"from scipy.stats import skew\n\ndef skewed(df, skewness_threshold=0.5, apply_transformation=True):\n    skewed_features = df.apply(lambda x: skew(x.dropna()))\n    skewed_features = skewed_features[abs(skewed_features) > skewness_threshold]\n    skewed_features = skewed_features.index\n\n    # Exclude categorical columns from skewness transformation\n    skewed_features = [feature for feature in skewed_features if df[feature].dtype != 'category']\n\n    if apply_transformation:\n        for feature in skewed_features:\n            df[feature] = np.log1p(df[feature])\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.806821Z","iopub.execute_input":"2024-04-05T19:12:52.807273Z","iopub.status.idle":"2024-04-05T19:12:52.821187Z","shell.execute_reply.started":"2024-04-05T19:12:52.807229Z","shell.execute_reply":"2024-04-05T19:12:52.820136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import skew\n\nsk_before = df.select_dtypes(include=['int64', 'float64']).apply(skew)\n\nsk_int = df.select_dtypes(include=['int64', 'float64']).apply(lambda x: np.log(x + 1))\n\nsk_after = sk_int.apply(skew)\n\ncomparison = pd.DataFrame({'Before': sk_before, 'After': sk_after})\n\nprint(comparison)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.822531Z","iopub.execute_input":"2024-04-05T19:12:52.823721Z","iopub.status.idle":"2024-04-05T19:12:52.929076Z","shell.execute_reply.started":"2024-04-05T19:12:52.823684Z","shell.execute_reply":"2024-04-05T19:12:52.927921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Skewness improved in the features where the skewness value after transformation moved closer to zero compared to before, indicating a distribution that is more symmetric. Based on the provided data, we can identify these improvements by looking for a decrease in the absolute value of skewness for negatively skewed distributions or an increase towards zero for positively skewed distributions, without crossing over to the opposite sign (which would indicate an over-correction). Specifically:\n\n- **MSSubClass**: Skewness decreased slightly, indicating a minor improvement towards a more symmetric distribution.\n- **LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd**: These features all show an increase in negative skewness, which might seem counterintuitive at first glance. However, without comparing the magnitude of change relative to the scale and context of each feature's distribution, determining improvement solely based on direction (positive or negative) may not be accurate for these cases.\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF**: All these features related to basement square footage show an increase in negative skewness, suggesting a further deviation from symmetry, hence not an improvement based on the general guideline.\n- **LowQualFinSF, 3SsnPorch, ScreenPorch, PoolArea, MiscVal**: The skewness values for these features remained very high, indicating that despite any minor changes, they remain heavily skewed.\n- **BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea**: These features' skewness changes vary, with some showing slight adjustments towards symmetry but still indicate the need for careful analysis to determine if the changes reflect genuine improvements towards normality or simply minor fluctuations.\n- **WoodDeckSF, OpenPorchSF, EnclosedPorch**: These features showed slight changes in skewness, indicating minor adjustments in their distributions.\n- **MoSold, YrSold**: Changes in these features are minimal, suggesting only slight adjustments in their distributions' symmetry.\n\nGiven the mixed nature of skewness changes, it's crucial to closely examine each feature's distribution and the scale of skewness changes. For this assessment, \"improvement\" was taken to mean any change that brings the skewness value closer to zero, reflecting a more symmetric distribution. However, for many features listed, the increase in negative skewness actually indicates a move away from symmetry, which would not typically be considered an improvement in skewness. Correct interpretation depends on the goals of the analysis and the specific transformation techniques applied.","metadata":{}},{"cell_type":"markdown","source":"# 5. Principal Component Analysis(PCA):","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Principal Component Analysis:\n\n\nPCA (Principal Component Analysis) is a technique used to reduce the dimensionality of high-dimensional datasets while preserving most of the original information. It transforms variables into a new set of uncorrelated variables called principal components, which capture the most important patterns in the data. PCA is essential for dimensionality reduction, noise reduction, feature extraction, visualization, and addressing multicollinearity issues in datasets.","metadata":{}},{"cell_type":"markdown","source":"`Princiap Component Analysis` computes the eigenvectors and eigenvalues of the dataset's covariance matrix, selects the top eigenvectors based on eigenvalues, and projects the data onto these components to reduce dimensionality.","metadata":{}},{"cell_type":"code","source":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.930691Z","iopub.execute_input":"2024-04-05T19:12:52.931266Z","iopub.status.idle":"2024-04-05T19:12:52.944816Z","shell.execute_reply.started":"2024-04-05T19:12:52.931221Z","shell.execute_reply":"2024-04-05T19:12:52.943618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pca_inspired(df):\n    X = pd.DataFrame()\n    # Composite feature 1: Addition of 'TotalBsmtSF' and 'GrLivArea'   \n    X['feature1'] = df.GrLivArea + df.TotalBsmtSF\n    # Composite feature 1: Multiplication of 'YearRemodAdd' and 'TotalBsmtSF'\n    X['feature2'] = df.YearRemodAdd * df.TotalBsmtSF\n  \n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\"\n]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.946379Z","iopub.execute_input":"2024-04-05T19:12:52.946727Z","iopub.status.idle":"2024-04-05T19:12:52.962560Z","shell.execute_reply.started":"2024-04-05T19:12:52.946698Z","shell.execute_reply":"2024-04-05T19:12:52.961303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Co-relation Matrix","metadata":{}},{"cell_type":"code","source":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method, numeric_only=True),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:52.964449Z","iopub.execute_input":"2024-04-05T19:12:52.964967Z","iopub.status.idle":"2024-04-05T19:12:54.692397Z","shell.execute_reply.started":"2024-04-05T19:12:52.964922Z","shell.execute_reply":"2024-04-05T19:12:54.691110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:54.694080Z","iopub.execute_input":"2024-04-05T19:12:54.694526Z","iopub.status.idle":"2024-04-05T19:12:54.708673Z","shell.execute_reply.started":"2024-04-05T19:12:54.694484Z","shell.execute_reply":"2024-04-05T19:12:54.707356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Handling Outliers - PCA Application","metadata":{}},{"cell_type":"markdown","source":"### 5.1.3 Interquartile Range (IQR) method","metadata":{}},{"cell_type":"markdown","source":"The code uses the **Interquartile Range (IQR) method** to handle outliers in numerical columns of a DataFrame. It first calculates the skewness of each column and then identifies outliers based on their deviation from the IQR. Outliers are capped at a certain threshold defined by 1.5 times the IQR above the third quartile or below the first quartile. Finally, the skewness of each column is recalculated to verify the effectiveness of outlier handling.","metadata":{}},{"cell_type":"code","source":"def indicate_outliers(df):\n    for column_name in df.select_dtypes(include=np.number).columns:\n        # Calculate skewness\n        skewness = df[column_name].skew()\n        print(\"Skewness of column '{}': {:.2f}\".format(column_name, skewness))\n        \n        # Check if skewness is beyond a certain threshold (e.g., 1 or -1)\n        if abs(skewness) > 1:\n            # Calculate interquartile range (IQR)\n            Q1 = df[column_name].quantile(0.25)\n            Q3 = df[column_name].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            # Define upper and lower bounds for outlier detection\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            \n            # Cap outliers\n            df[column_name] = np.where(df[column_name] < lower_bound, lower_bound, df[column_name])\n            df[column_name] = np.where(df[column_name] > upper_bound, upper_bound, df[column_name])\n            \n            print(\"Outliers handled for column '{}'.\".format(column_name))\n            \n           \n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:54.710508Z","iopub.execute_input":"2024-04-05T19:12:54.711625Z","iopub.status.idle":"2024-04-05T19:12:54.728768Z","shell.execute_reply.started":"2024-04-05T19:12:54.711581Z","shell.execute_reply":"2024-04-05T19:12:54.727454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Holdout Validation","metadata":{}},{"cell_type":"markdown","source":"1. Split the dataset into two subsets: a training set and a validation set.\n2. Allocate approximately 80% of the data to the training set and 20% to the validation set.\n3. Train the machine learning model using the training set.\n4. Evaluate the model's performance using the validation set.\n5. Assess the model's performance metrics, such as accuracy, precision, recall, or F1 score.\n6. Adjust the model's hyperparameters based on the validation performance.\n7. Repeat the training and evaluation process with the updated hyperparameters.\n8. Continue iterating until satisfactory performance is achieved on the validation set.\n9. Ensure that the model generalizes well to unseen data by validating its performance on the validation set.","metadata":{}},{"cell_type":"code","source":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    \n    X = X.join(interaction_3(X), lsuffix='_existing', rsuffix='_new')\n\n\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n    X = indicate_outliers(X)\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\ndf = df[df.GrLivArea < 4000]  #removing outliers per the information file guidance\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:54.736138Z","iopub.execute_input":"2024-04-05T19:12:54.736583Z","iopub.status.idle":"2024-04-05T19:12:58.418131Z","shell.execute_reply.started":"2024-04-05T19:12:54.736546Z","shell.execute_reply":"2024-04-05T19:12:58.416910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.info())","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:58.419990Z","iopub.execute_input":"2024-04-05T19:12:58.420717Z","iopub.status.idle":"2024-04-05T19:12:58.448557Z","shell.execute_reply.started":"2024-04-05T19:12:58.420673Z","shell.execute_reply":"2024-04-05T19:12:58.447405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Model Comparison","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Gradient Boosted Decision Trees (GBDTs)\n","metadata":{}},{"cell_type":"markdown","source":"### XGBoost: eXtreme Gradient Boosting\n\n### LightGBM: Light Gradient Boosting Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport lightgbm as lgb\n\n# Assuming df_train contains your training data\nX_train = create_features(df_train)\ny_train = df_train['SalePrice']\n\n# Define numerical and categorical features\nnumerical_features = df_train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = df_train.select_dtypes(include=['object']).columns\n# Define XGBoost parameters\nxgb_params = {\n    'max_depth': 3,\n    'learning_rate': 0.001,\n    'n_estimators': 6000,\n    'min_child_weight': 1,\n    'colsample_bytree': 0.7,\n    'subsample': 0.7,\n    'reg_alpha': 0.5,\n    'reg_lambda': 1.0,\n    'num_parallel_tree': 1\n}\n\n# Initialize XGBoost model\nxgb = XGBRegressor(**xgb_params)\n\n# Score dataset using XGBoost\nscore_dataset(X_train, y_train, xgb)\n\n# Define LightGBM parameters\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\n# Train the LightGBM model\nmodel = lgb.LGBMRegressor(**lgb_params)  # Assuming you have already obtained the best_params\nmodel.fit(X_train, y_train)\n\n# Score dataset using cross-validation\nscore_dataset(X_train, y_train, model)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:12:58.450655Z","iopub.execute_input":"2024-04-05T19:12:58.451143Z","iopub.status.idle":"2024-04-05T19:13:51.402645Z","shell.execute_reply.started":"2024-04-05T19:12:58.451099Z","shell.execute_reply":"2024-04-05T19:13:51.401407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = lgb.LGBMRegressor(**lgb_params)\n\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:13:51.403967Z","iopub.execute_input":"2024-04-05T19:13:51.404573Z","iopub.status.idle":"2024-04-05T19:13:52.897278Z","shell.execute_reply.started":"2024-04-05T19:13:51.404540Z","shell.execute_reply":"2024-04-05T19:13:52.895994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Conclusion","metadata":{}},{"cell_type":"markdown","source":"**In this project, I opted for LightGBM (LGBM) and found it delivered better results compared to XGBoost (XGBoost), particularly for my dataset of 3000 records. Despite LGBM's known efficiency with categorical features, I still applied preprocessing steps, specifically label encoding, to ensure consistency across models. LGBM's inherent ability to handle categorical data efficiently and its faster training times contributed significantly to the improved performance. This approach allowed for quicker iteration and more extensive hyperparameter tuning, giving LGBM an edge.\nWith XGBoost, I achieved a score of `0.13374` on the public leaderboard. However, with LightGBM, the score improved to `0.12824`, indicating its superior predictive capability in this particular context.\nAlthough XGBoost is a powerful tool, requiring additional preprocessing and more detailed tuning efforts might have limited its performance in my specific case, leading to LGBM's superior outcome.**\n","metadata":{}}]}